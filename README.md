# Northeastern University Math 7243 - Machine Learning 1

This course introduces both the mathematical theory of learning and the implementation of modern machine-learning algorithms appropriate for data science. Modeling everything from social organization to financial predictions, machine-learning algorithms allow us to discover information about complex systems, even when the underlying probability distributions are unknown. Algorithms discussed include regression, decision trees, clustering, neural networks and dimensionality reduction techniques. The course offers students an opportunity to learn the implications of the mathematical choices underpinning the use of each algorithm, how the results can be interpreted in actionable ways, and how to apply their knowledge through the analysis of a variety of data sets and models. 

## Lectures

| Lecture Number    | Title          | Topics |
| :---         |     :---:      |          ---: |
| [Lecture 1](https://drive.google.com/file/d/1_rZ-bPs22E-WaCYkMqSKSBkGPXPuVGn0/view?usp=sharing)    | Introduction to Machine Learning | What is machine learning, terminology and notation, first examples: linear regression and binary classification with k-nearest neighbors |
| [Lecture 2](https://drive.google.com/file/d/1aPtUh7dQrup-dUYDxHvRTz0Kz2Bin-9B/view?usp=sharing)    | Matrix Differentiation and Optimization    | Brief introduction to the bias-variance trade off and the curse of dimensionality, matrix differentiation   |
| [Lecture 3](https://drive.google.com/file/d/17jJe9V4ZyUIzgs0BmhZyNnmCMVyhgkM_/view?usp=sharing)    | Linear Regression    | Variance of linear parameters, confidence intervals and z-scores for linear parameters, feature selection via statistical significance, subset selection   |
| [Lecture 4](https://drive.google.com/file/d/1njf56HJQV1AeplvcbAuCiAC9m8Q04lYS/view?usp=sharing)    | Parameter Shrinkage Methods | Gauss-Markov Theorem, Ridge Regression, Lasso Regression, Degrees of Freedom |
| [Lecture 5](https://drive.google.com/file/d/13NnBP3FjBQNYmjuxt6tI6uAohLFs7rUq/view?usp=sharing)    | Linear Methods in Classification | Multilabel Classification, Regression on Categorical Variables,	Linear Discriminant Analysis, Logistic Regression, Fitting Logistic Regression with Newtons Method, Extra: Bayes Classifier |
| [Lecture 6](https://drive.google.com/file/d/11D6e1-Ecit8mYwHOEnCsKJCk3U0XeCQc/view?usp=sharing)    | Iterative Methods | Gradient Decent, Stochastic Gradient Decent, Newtons Method, Example: Polynomial Fitting, Example: Fitting Nonpolynomial Functions|
| [Lecture 7](https://drive.google.com/file/d/1CtdTh_fg84G183R7GNPfHy2QOcPlVqNf/view?usp=sharing)    | Smoothing Methods | Piecewise Polynomials and Splines, Endpoint Selection and Smoothing Splines, Multidimensional Splines, Kernel Smoothing, Other Bases|
| [Lecture 8](https://drive.google.com/file/d/1AGRU3fl9wwFj8XR5tXVlA9HC1Oag568a/view?usp=sharing)    | Artificial Neural Networks | Artificial Neural Networks, Linear Classifier, Neural Networks and the Perceptron, Multilabel Perceptrons, Gradient Decent and Back Propagation, Back Propagation|
| [Lecture 9](https://drive.google.com/file/d/1vGknYucamOCo7dx-yllMcupQ9o0oynIF/view?usp=sharing)    | Convolutional Neural Networks | Types of Artificial Neural Networks, Convolutional Neural Networks, History of CNNs, Using Pretrained CNN’s|
| [Lecture 10](https://drive.google.com/file/d/1HYvTs1p_ip7Dh7Ne59WjH96JIgqvkWkk/view?usp=sharing)   | Recurrent Neural Networks | Types of Artificial Neural Networks, Recurrent Networks, Recurrence Nodes, Applying RNN’s to Natural Language Processing, Extra: Symmetrically Connected Networks|
| [Lecture 11](https://drive.google.com/file/d/1lG_K_Uy7dCyGPz8_MCa63uIJOHafUYdN/view?usp=sharing)   | Training Deep Networks | Vanishing Gradients and Activation Functions, Batch Normalization and Gradient Clipping, Faster Optimizers, Regularization |
| [Lecture 12](https://drive.google.com/file/d/1aS81JPYestYibXcGKp9FV5GeJvHQxF7E/view?usp=sharing)   | Factor Analysis and PCA | Feature Construction and Dimensional Reduction, Exploratory Factor Analysis, Principle Component Analysis, Nonlinear PCA|
| [Lecture 13](https://drive.google.com/file/d/1J0q9kwjjeJxqtWDdGdFoyOCRehslNjc1/view?usp=sharing)   | Decision Trees and Support Vector Machines | Decision Trees, Support Vector Machines, SVM’s and the Kernel Trick|
| [Lecture 14](https://drive.google.com/file/d/1jOlut-Rb7U-ZjlJzh46lyTg-I1ArICzd/view?usp=sharing)   | Cluster Analysis Part 1 | Overview of Clustering, K-Means Clusters, Example: Gene Expression Clustering, Density Based Clustering, Dissimilarity |
| [Lecture 15](https://drive.google.com/file/d/1vueQ1rcYOjCt9GCbIhP8Wn_7AmvVOHkL/view?usp=sharing)   | Cluster Anaylsis Part 2 | Dissimilarity, Agglomerative Clustering, Divisive Clustering, Spectral Clustering |
| [Lecture 16](https://drive.google.com/file/d/1lJ-Jekt1fkPDQU9vBKP8Kx2BSlk8L_JL/view?usp=sharing)   | Cluster Analysis Part 3 | Spectral Clustering, Gaussian Mixture Models, Evaluating Clustering: Internal and External Measures, The Theoretical Problem of Clustering |
| [Lecture 17](https://drive.google.com/file/d/1RG_VRoMXzvGeMZAagR4MAP-g8ScxfbEa/view?usp=sharing)   | Boosting, Bagging and Bootstrapping | Boosting and Adaboost, Adaboost in the Loss Minimization Framework, Boosting Trees, Bootstraping Confidence Intervals, Bagging and Bumping|
| [Lecture 18](https://drive.google.com/file/d/1taJigPyzH4zQAoZi3YbzQ9llkIPamwIz/view?usp=sharing)   | Mathematical Foundations of Machine Learning | Formal Model for Statistical Learning, PAC and APAC Learning, Uniform Convergence, Hoeffding’s Inequality, Finite Hypothesis Classes are APAC Learnable |
| [Lecture 19](https://drive.google.com/file/d/1jtbOefxxTX8iunEVaMDIvDTXUW-iDsL0/view?usp=sharing)   | No Free Lunch | Proving Bounds in APAC, Finite Hypothesis Classes are APAC Learnable, No Free Lunch Theorem, The Bias-Variance Tradeoff for RSS, Examples: k nearest neighbors and Linear Predictor|
| [Lecture 20](https://drive.google.com/file/d/1yGDpXlmgvjApBLeLrMASCw2-I4DngEpg/view) | The Vapnik-Chervonenkis Dimension | The class of all functions is unlearnable, VC Dimension with Examples, The Fundamental Theorem of PAC Learning, Sauers Lemma, Examples: Boosting and PAC learning, VC-Dimension of Neural Networks|
| [Fin](https://drive.google.com/file/d/1zPPdOJbNXTzl8InccuN7ih-AleH6RNzF/view?usp=sharing) | Survey Of Further Directions | What We Have Done, Topics We Missed, Where To Go From Here |

### Additional Lectures

| Lecture Number    | Title          | Topics |
| :---         |     :---:      |          ---: |
| Lecture 4.5    | Dimensional Reduction | Feature construction, dimensional reduction, factor analysis, PCA |


## Labs

| Lab Number    | Title          | Topics |
| :---         |     :---:      |          ---: |
| Lab 1    | Exploratory Analysis | Loading CSV files, data frames,  graphing, exploratory analysis, linear regression. |
| Lab 2    | Linear Regression | Linear regression with linear algebra, sklearn and statsmodel.api; ridge and lasso regression; subset selection methods. |
| Lab 3    | Linear Methods in Classification | Visualization of categorically labeled data, categorical linear regression, logistic regression, linear and quadratic discriminant analysis. |
| Lab 4 | Artificial Neural Networks with Tensorflow and Keras | Building, training, using and saving ANN's using Keras with a Tensorflow backend. |
| Lab 5 | Convolutional Neural Networks with Keras | Building and training CNN's, using pretrained CNN's, using Keras. |
| Lab 6 | Recurrent Neural Networks with Keras | Building and training RNN's for sequence generation and text generation. |


## Hosted External Datasets

| Dataset   | Title          | Lab |
| :---         |     :---:      |          ---: |
| Ames Housing Prices  | Ames | Lab 1 |
| NYC AirBnB Prices | NYCAirBnB | Lab 1 |

## Project Gallery

 <div class="row">
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Boston%20311%20Mixture%20Model%20Fitting%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Predicting Probability of 311 Service Request Categories</b></h4>
        <p>Jon Gray</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Single%20Channel%20Speech%20Enhancement%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Single Channel Speech Enhancement</b></h4>
        <p>Andrew Stockton</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
 </div>
 
 <div class="row">
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/IPO%20Success%20and%20Failure.png" style="width:100%">
      <div class="container">
        <h4><b>Predict IPOs Success/Failure</b></h4>
        <p>Mingfeng Gao,  Xingjian Huang,  Haojun Cao</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Stock%20Price%20Prediction%20from%20Reddit%20News%20and%20Market%20Information%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Stock Price Prediction from Reddit News and Market Information</b></h4>
        <p>Pranshu Tiwari</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
 </div>
 
 
 <div class="row">
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/3d%20Segmentation%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>MRI Image Segmentation</b></h4>
        <p>Ruobing Bai, Sara Benedetti, Yakun Chen, Chun-Li Chuang, Wanchen Geng, Ruiwen Jin, Rohit Thakur, Zheying Yu</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Boston%20311%20Violation%20Prediction%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Boston 311 Violation Prediction</b></h4>
        <p>Emily Obudzinski, Taylor Ketterer, Edith Aromando, Alison Abrams</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
 </div>
 
 <div class="row">
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Side%20Channel%20Attack%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>New loss function of Deep Learning based Side-Channel Attack</b></h4>
        <p>Ziyue Zhang, Xiang Zhang</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Word%20Embeddings%20for%2020%20Newsgroups%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Word Embeddings for 20 Newsgroups</b></h4>
        <p>Matthew Burchfield</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
 </div>
 
  <div class="row">
  <div class="column">
    <div class="card">
      <img src="https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Projects/Predicting%20VIX%20Ticker%20Volatility%20from%20Headlines%20Thumb.png" style="width:100%">
      <div class="container">
        <h4><b>Predicting VIX Ticker Volatility from Headlines</b></h4>
        <p>Joshua Galloway</p>
        <p><a>Paper</a> | <a>Presentation</a> | <a>Slides</a></p>
      </div>
    </div>
  </div>
 </div>


